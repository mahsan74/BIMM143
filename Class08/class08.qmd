---
title: "Class 08"
format: pdf
author: Shayan Malekpoor
---

### 1. Preparing the data

```{r}
#import the data to R studio

fna.data <- "WisconsinCancer.csv"

wisc.df <- read.csv(fna.data, row.names=1)

head(wisc.df)
```

```{r}
#removing first column

wisc.data <- wisc.df[,-1]
```

```{r}
# form a new data frame and call it diagnosis

diagnosis <- as.factor(wisc.df$diagnosis)
```

### Exploring data analysis

**Q1**. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

Q2. How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)
```

Q3: How many variables/features in the data are suffixed with `_mean`?

```{r}
 grep("_mean", colnames(wisc.df))
```

### 2. Principal Component Analysis

```{r}
# using colMean and apply for PCA

colMeans(wisc.data)
```

```{r}
apply(wisc.data,2,sd)
```

```{r}
wisc.pr <- prcomp(scale(wisc.data))
```

```{r}
# we get the summery

summary(wisc.pr)
```

**Q4**. From your results, what proportion of the original variance is captured by the first principal components (PC1)? 0.4427

**Q5**. How many principal components (PCs) are required to describe at least 70% of the original variance in the data? at least 3 PCs

**Q6**. How many principal components (PCs) are required to describe at least 90% of the original variance in the data? at least 7 PCs

### Interpreting PCA results

```{r}
biplot(wisc.pr)
```

Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why? the plot is very crowded. it is difficult to interpret the data because number of observations are too much.

```{r}
# we can generate more standard scatter plot
plot( wisc.pr$x[,1:2] , col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
```

Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots? There is less separation between malignant and benign.

```{r}
plot(wisc.pr$x[,c(1,3)], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")

```

```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
```

```{r}
# We can use ggplot for better visualization

library(ggplot2)
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) +  geom_point()
```

### Variance explained

```{r}
# calculate the variance of each PC
pr.var <- wisc.pr$sdev^2
pr.var
```

```{r}
pve <- pr.var / sum(pr.var)
pve
```

```{r}
# plotting variance explained for each PC
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# We can plot this data another way
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

### Communicating PCA results

Q9. For the first principal component, what is the component of the loading vector? contribution of original feature to first PC.

```{r}
wisc.pr$rotation["concave.points_mean",1]
```

```{r}
sort(wisc.pr$rotation[,1])
```

### 3. Hierarchical clustering

```{r}
# clustering 
data.scaled <- scale(wisc.data)
```

```{r}
# calculate the distance
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist, method="complete")
```

### Results of hierarchical clustering

Q10. Using the `plot()` and `abline()` functions, what is the height at which the clustering model has 4 clusters? 19

```{r}
plot(wisc.hclust)
abline(h = 19, col="red", lty=2)
```

### Selecting number of clusters

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

### Using different methods

**Q12.** Which method gives your favorite results for the same `data.dist` dataset? Explain your reasoning ward.D2 decrease the total variance, that result better clusters.

### 4. Combining methods

### Clustering on PCA results

```{r}
cum_var <- cumsum(wisc.pr$sdev^2 / sum(wisc.pr$sdev^2))
```

```{r}
n_pc <- min(which(cum_var >= 0.9))
n_pc
```

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:n_pc]), method = "ward.D2")
```

```{r}
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
wisc.pr.hclust
```

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

```{r}
plot(wisc.pr.hclust, hang = -1)
```

```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```

**Q13**. How well does the newly created model with four clusters separate out the two diagnoses? separated clusters are looking good.

**Q14**. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? the wisc.hclust.clusters with 4 clusters should be used due to better separation.

```{r}
wisc.hclust <- hclust(dist(wisc.data[, -1]))
wisc.hclust.clusters <- cutree(wisc.hclust, k=2)

```

```{r}
wisc.data <- read.csv("WisconsinCancer.csv")
wisc.km <- kmeans(wisc.data[,3:32], centers = 2)
```

```{r}
wisc.hclust <- hclust(dist(wisc.data[,3:32]), method="ward.D2")
wisc.hclust.clusters <- cutree(wisc.hclust, k=2)
```

```{r}
table(wisc.km$cluster, wisc.data$diagnosis)
table(wisc.hclust.clusters, wisc.data$diagnosis)
```

### 6. Prediction

```{r}
#url <- "new_samples.csv"
newcancer_cell_data <- "new_samples.csv"
new <- read.csv("https://tinyurl.com/new-samples-CSV")
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

**Q16.** Which of these new patients should we prioritize for follow up based on your results? The patient closer to the malignant cluster
